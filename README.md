# TG Crawler

Краулер для сбора и анализа постов с Telegram каналов на примере [digitaltender](https://t.me/s/digitaltender).

## Особенности

- ✅ **TypeScript**
- ✅ **Playwright**
- ✅ **Hono** 
- ✅ **Пагинация**
- ✅ **Фильтрация рекламы** 
- ✅ **REST API** 
- ✅ **Сохранение данных** 

## Установка

```bash
npm install
```

## Запуск

### Запуск сервера с API

```bash
npm run dev
```

Сервер будет доступен на `http://localhost:3000`

### Только краулер

```bash
npm run crawler
```

## API Endpoints

### GET / 
Информация о сервисе

### GET /api/jobs
Получить все собранные посты

**Ответ:**
```json
{
  "success": true,
  "count": 42,
  "jobs": [...]
}
```

### GET /api/jobs/:id
Получить пост по ID

### GET /api/jobs/filter/type/:type
Фильтровать посты по типу работы

Примеры:
- `/api/jobs/filter/type/разовая`
- `/api/jobs/filter/type/постоянная`

### POST /api/crawl
Запустить краулер

**Ответ:**
```json
{
  "success": true,
  "message": "Собрано 42 постов",
  "posts": [...]
}
```

### GET /api/stats
Получить статистику по типам работ и оплате

## Структура проекта

```
src/
├── index.ts        # Основной сервер с API
├── crawler.ts      # Логика краулинга Telegram
├── adFilter.ts     # Фильтрация рекламы и спама
└── storage.ts      # Сохранение и загрузка данных

data/
└── jobs_YYYY-MM-DD.json   # Сохраненные посты
```

## Структура поста

```typescript
interface JobPost {
  id: string;                    // Уникальный ID поста
  title: string;                 // Заголовок
  description: string;           // Полное описание
  workType: string;              // Тип работы (разовая, постоянная и т.д.)
  payment: string;               // Информация об оплате
  deadline: string;              // Сроки выполнения
  url: string;                   // URL поста
  scrapedAt: string;             // Время сбора (ISO 8601)
}
```

## Примеры использования

### cURL

```bash
# Получить все посты
curl http://localhost:3000/api/jobs

# Запустить краулер
curl -X POST http://localhost:3000/api/crawl

# Получить статистику
curl http://localhost:3000/api/stats

# Фильтровать по типу работы
curl "http://localhost:3000/api/jobs/filter/type/разовая"
```

### JavaScript/Fetch

```javascript
// Запустить краулер
const response = await fetch('http://localhost:3000/api/crawl', {
  method: 'POST'
});
const data = await response.json();
console.log(`Собрано ${data.message}`);

// Получить все посты
const jobs = await fetch('http://localhost:3000/api/jobs');
const { jobs: allJobs } = await jobs.json();
```

## Фильтрация рекламы

Краулер автоматически фильтрует:
- Посты с пометкой "[реклама]"
- Спам и рассылки
- Содержимое ботов
- Короткие посты без ключевых слов о работе

Ищет в постах:
- Ключевые слова: вакансия, работа, соискатель, специалист, услуга
- Структурированные данные: тип работы, оплата, сроки

## Особенности краулера

1. **Умная пагинация** - автоматически определяет конец контента
2. **Парсинг структуры** - извлекает структурированные данные из постов
3. **Дедупликация** - избегает дублей при сохранении
4. **Обработка ошибок** - продолжает работу при сбое на отдельном посте

## Лимиты

- По умолчанию краулер делает максимум 10 итераций (можно изменить)
- Задержка между скроллингом: 1 секунда
- Timeout для загрузки страницы: 30 секунд

## Возможные улучшения

- [ ] Добавить прокси поддержку
- [ ] Реализовать очередь краулинга
- [ ] Добавить уведомления о новых постах
- [ ] Реализовать поиск по ключевым словам
- [ ] Добавить экспорт в CSV/Excel
- [ ] Реализовать мониторинг качества данных

## Лицензия

MIT
